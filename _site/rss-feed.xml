<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/">
	<channel>
		<title>Disorder Transform</title>
		<description>Math, physics, life, universe and everything</description>
		<link></link>
		<atom:link href="/rss-feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Directed Graphs and Conditional Independence</title>
				
					<dc:creator>Junior A. Koch</dc:creator>
				
				
					<description>&lt;h2 id=&quot;the-bias-variance-tradeoff&quot;&gt;The Bias-Variance Tradeoff&lt;/h2&gt;

</description>
				
				<pubDate>Thu, 23 Jan 2020 00:00:00 -0300</pubDate>
				<link>http://localhost:4000/journal/binning.html</link>
				<guid isPermaLink="true">http://localhost:4000/journal/binning.html</guid>
			</item>
		
			<item>
				<title>Directed Graphs and Conditional Independence</title>
				
					<dc:creator>Junior A. Koch</dc:creator>
				
				
					<description>&lt;p&gt;A Directed Graph is nothing more than a set of nodes and arrows. Nodes represent variables and arrows represent relationship between such variables. Directed Graphs can be used in similar way as Counter Factuals in order to represent causal relationship. When a graph has the meaning of a probability distributions it is usally called a Bayesian Network, although not everyone likes this terminology.&lt;/p&gt;

</description>
				
				<pubDate>Wed, 15 Jan 2020 00:00:00 -0300</pubDate>
				<link>http://localhost:4000/journal/dags-cond_indep.html</link>
				<guid isPermaLink="true">http://localhost:4000/journal/dags-cond_indep.html</guid>
			</item>
		
			<item>
				<title>Information Theory</title>
				
					<dc:creator>Junior A. Koch</dc:creator>
				
				
					<description>&lt;p&gt;Suppose we have a random variable &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; and we are interested in how much information we get when &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is measured. If the event is highly improbable we have more information than if it is highly probable. Information is given by a quantity &lt;script type=&quot;math/tex&quot;&gt;h(x)&lt;/script&gt; that is a function of the probability distribution &lt;script type=&quot;math/tex&quot;&gt;p(x)&lt;/script&gt;. If two events are observed and unrelated the total information will be the sum of information in both events:&lt;/p&gt;

</description>
				
				<pubDate>Sat, 19 Oct 2019 00:00:00 -0300</pubDate>
				<link>http://localhost:4000/journal/information-theory.html</link>
				<guid isPermaLink="true">http://localhost:4000/journal/information-theory.html</guid>
			</item>
		
			<item>
				<title>Counter Factual Model</title>
				
					<dc:creator>Junior A. Koch</dc:creator>
				
				
					<description>&lt;h1 id=&quot;inferência-causal&quot;&gt;Inferência Causal&lt;/h1&gt;

</description>
				
				<pubDate>Sun, 01 Sep 2019 00:00:00 -0300</pubDate>
				<link>http://localhost:4000/journal/counter-factual_model.html</link>
				<guid isPermaLink="true">http://localhost:4000/journal/counter-factual_model.html</guid>
			</item>
		
	</channel>
</rss>
