<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-09-18T18:31:00-03:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Disorder Transform</title><subtitle>Math, physics, life, universe and everything</subtitle><author><name>Junior A. Koch</name></author><entry><title type="html">Introdução ao Manifold Learning - Geometria dos dados</title><link href="http://localhost:4000/journal/intro-manifold.html" rel="alternate" type="text/html" title="Introdução ao Manifold Learning - Geometria dos dados" /><published>2020-09-18T00:00:00-03:00</published><updated>2020-09-18T00:00:00-03:00</updated><id>http://localhost:4000/journal/intro-manifold</id><content type="html" xml:base="http://localhost:4000/journal/intro-manifold.html">&lt;p&gt;Muito do trabalhado de um Cientista de Dados não está apenas em criar sistemas de classificação, regressão ou recomendação. Aliás, estes são normalmente o resultado do fim de uma grande jornada aplicando diversas técnicas em cima dos dados coletados. Uma importante parte dessa jornada está em descobrir maneiras de compreender e utilizar a estrutura dos dados.&lt;/p&gt;

&lt;p&gt;Eu tenho um carinho especial por esse tipo de abordagem. Minha formação é como físico e boa parte da compreensão que temos do Universo veio através dessa estrutura dos dados observados. Vou dar um exemplo: compreendemos o Universo como uma espécie de superfície (uma superfície complicada, mas ainda sim uma superfície), nós vivemos sobre ela e qualquer coisa que possamos fazer deve obedecer um conjunto de regras (Leis Físicas) que são determinadas, a grosso modo, pelo formato da superfície, ou seja, pela geometria. Se há uma estrela em determinado lugar, a massa dela curva o espaço e a regra a que estamos submetidos é ser atraído para a estrela.&lt;/p&gt;

&lt;p&gt;O conjunto gigante de dados que observamos todos os dias pertence a algum universo próprio, com regras próprias e entender as regras desse ambiente nos permite entender como obter melhores respostas através dos dados. Temos hoje diversas maneiras de realizar este tipo de tarefa e aqui vou falar sobre uma chamada Manifold Learning ou Aprendizagem de Variedade. Primeiro precisamos entender o que é uma Variedade (Manifold em inglês).&lt;/p&gt;

&lt;h2 id=&quot;algumas-definições&quot;&gt;Algumas definições&lt;/h2&gt;

&lt;p&gt;Essa parte pode parecer chata se você não gosta de matemática, mas é essencial definirmos alguns termos para compreender o que há pela frente.&lt;/p&gt;

&lt;p&gt;Primeiro precisamos do conceito de &lt;strong&gt;mapa diferenciável&lt;/strong&gt; (Um &lt;strong&gt;mapa&lt;/strong&gt; é apenas uma definição mais abrangente de função):&lt;br /&gt;
Seja um mapa &lt;script type=&quot;math/tex&quot;&gt;f:U \rightarrow V&lt;/script&gt;, onde &lt;script type=&quot;math/tex&quot;&gt;U \subset \mathbb{R}^{n}&lt;/script&gt; e &lt;script type=&quot;math/tex&quot;&gt;V \subset \mathbb{R}^{k}&lt;/script&gt;. Cada um deles, &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt; e &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;, é um subconjunto de algum espaço Euclidiano (dois espaços com dimensões não necessariamente iguais). Dizemos que o mapa &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; é suave ou diferenciável se para qualquer &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; as derivadas&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial^{m}f}{\partial x_{i}...\partial x_{m}}&lt;/script&gt;

&lt;p&gt;formam um mapa suave, ou seja, se as derivadas existem em todos os pontos (não há divisões por zero ou descontinuidades).&lt;/p&gt;

&lt;p&gt;Sejam &lt;script type=&quot;math/tex&quot;&gt;X \subset \mathbb{R}^{n}&lt;/script&gt; e &lt;script type=&quot;math/tex&quot;&gt;Y \subset \mathbb{R}^{k}&lt;/script&gt; subconjuntos quaisquer. O mapa &lt;script type=&quot;math/tex&quot;&gt;f: X \rightarrow Y&lt;/script&gt; é suave se para cada &lt;script type=&quot;math/tex&quot;&gt;x\in X&lt;/script&gt; existe uma vizinhança aberta &lt;script type=&quot;math/tex&quot;&gt;U \subset \mathbb{R}^{n}&lt;/script&gt; (uma região de pontos vizinhos a &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; que não é um intervalo fechado) e um mapa suave &lt;script type=&quot;math/tex&quot;&gt;g: U \rightarrow \mathbb{R}^{k}&lt;/script&gt; que coincida com &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; na intersecção &lt;script type=&quot;math/tex&quot;&gt;U \cap X&lt;/script&gt;. Isso basicamente quer dizer que pode não existir um único mapa que mapeie todos os pontos de &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; para &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;, mas se existir um conjunto de mapas que façam isso, então o conjunto é equivalente a um mapa suave.&lt;/p&gt;

&lt;p&gt;Dizemos que um mapa &lt;script type=&quot;math/tex&quot;&gt;f: X \rightarrow Y&lt;/script&gt; é um &lt;strong&gt;homeomorfismo&lt;/strong&gt; se é uma &lt;a href=&quot;https://pt.wikipedia.org/wiki/Fun%C3%A7%C3%A3o_bijectiva&quot;&gt;bijeção&lt;/a&gt; com &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; e sua inversa &lt;script type=&quot;math/tex&quot;&gt;f^{-1}&lt;/script&gt; &lt;strong&gt;contínuas&lt;/strong&gt;. No caso de &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; e &lt;script type=&quot;math/tex&quot;&gt;f^{-1}&lt;/script&gt; serem ambas suaves, dizemos que o homeomorfismo é um &lt;strong&gt;difeomorfismo&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Agora, uma variedade é um espaço topológico (basicamente um espaço em que conseguimos medir a distância entre dois pontos, pense em uma superfície qualquer) que localmente é difeomórfico a algum espaço Euclidiano, ou seja, há um conjunto de mapas capaz de projetar todos os pontos da variedade para o espaço Euclidiano que ao mesmo tempo possuem inversas contínuas e suaves. O termo localmente aqui significa &lt;em&gt;na vizinhança de algum ponto&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;A chave aqui está na curvatura desse espaço, podemos entender uma variedade como um espaço topológico que ao olhado de perto (dando um zoom) não veremos curvatura, será um plano.&lt;/p&gt;

&lt;p&gt;Só nos falta agora a definição de variedade diferenciável. Seja &lt;script type=&quot;math/tex&quot;&gt;M \subset \mathbb{R}^{n}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt; é uma variedade diferenciável de dimensão &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; se para cada ponto &lt;script type=&quot;math/tex&quot;&gt;x \in M&lt;/script&gt; existir uma vizinhança aberta &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt; contendo &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; e um difeomorfismo &lt;script type=&quot;math/tex&quot;&gt;f:U \rightarrow V&lt;/script&gt; onde &lt;script type=&quot;math/tex&quot;&gt;V \subset \mathbb{R}^{d}&lt;/script&gt;. Essas vizinhanças são chamadas de &lt;strong&gt;patches&lt;/strong&gt; coordenados e os difeomorfismos de cartas coordenadas (ou apenas sistemas de coordenadas, coordinate chart em inglês).&lt;/p&gt;

&lt;p&gt;Vamos a um exemplo: Pense em um círculo desenhado em um plano. O círculo é uma superfície de dimensão 1 &lt;strong&gt;merguhada&lt;/strong&gt; (&lt;strong&gt;embedded&lt;/strong&gt;) dentro do plano de duas dimensões, o &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^{2}&lt;/script&gt;. Cada um dos pontos desse círculo podem ser projetados no &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^{2}&lt;/script&gt; com as equações&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;x = r \cdot cos(\theta),\ \ y = r \cdot sen(\theta)&lt;/script&gt;,&lt;/p&gt;

&lt;p&gt;funções do ângulo &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; e do raio &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt;. Para fazer a transformação inversa precisamos das equações&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
r = \sqrt{x^{2}+y^{2}},\ \ \theta = \left\{\begin{matrix}
 arctan(y/x) &amp; se\ y&gt;0,\\ 
 \pi &amp; se\ y=0,\ x&lt;0,  \\
 \pi + arctan(y/x) &amp; se\ y&lt;0. \\  
\end{matrix}\right. %]]&gt;&lt;/script&gt;

&lt;p&gt;Observe que temos três possíveis mapas de &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; para que possamos cobrir todo o círculo, três patches e três funções. Os mapas podem não ser suaves em algum ponto, mas há outro mapa suave nesse ponto. Assim cobrimos tudo e o círculo dotado dessas projeções, ou mapas diferenciáveis formam uma variedade de dimensão 1 mergulhada em um espaço Euclidiano de dimensão 2.&lt;/p&gt;

&lt;h2 id=&quot;objetivo-de-manifold-learning&quot;&gt;Objetivo de Manifold Learning&lt;/h2&gt;

&lt;p&gt;De posse desses conceitos podemos entender um conceito bem famoso em Ciência de Dados chamado de &lt;strong&gt;Hipótese da Variedade&lt;/strong&gt;. Ela afirma que os dados observados(quaisquer que sejam) pertencem à uma variedade, ou seja, são pontos coletados desta superfície e com um número suficiente de pontos podemos recriar a variedade ou aprender suas propriedades. É isso que chamamos de &lt;strong&gt;Manifold Learning&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Seja o dataset &lt;script type=&quot;math/tex&quot;&gt;X = \lbrace x_{1}, ..., x_{n}\rbrace \subset \mathbb{R}^{D}&lt;/script&gt;, cada data point &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; é um vetor de dimensão $D$ (as vezes chamado de vetor de features). Assumimos que os dados pertencem à uma variedade de dimensão &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; mergulhada em &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^{D}&lt;/script&gt;, onde &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
d &lt; D %]]&gt;&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;A tarefa que queremos realizar é: Dado que &lt;script type=&quot;math/tex&quot;&gt;X \subset \mathbb{R}^{D}&lt;/script&gt; tal que &lt;script type=&quot;math/tex&quot;&gt;X \subset M&lt;/script&gt;, encontre a estrutura da variedade &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt;, isto é, a coleção de patches coordenados e cartas coordenadas.&lt;/p&gt;

&lt;p&gt;Os patches de um conjunto discreto são construídos encontrando os &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; vizinhos mais próximos (knn, k-nearest neighbors) de cada ponto &lt;script type=&quot;math/tex&quot;&gt;x \in X&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;N_{k}(x_{i}) = \lbrace x \in X \vert x\ \text{é um knn de } x_{i} \rbrace&lt;/script&gt;

&lt;p&gt;Dados os patches o objetivo é descobrir os difeomorfismos:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;f_{i}: N_{k}(x_{i}) \rightarrow U_{i} \subset \mathbb{R}^{k}&lt;/script&gt;,&lt;/p&gt;

&lt;p&gt;ou seja, as funções que mapeiam &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; e seus vizinhos para vizinhanças mergulhadas no &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^{k}&lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;e-agora&quot;&gt;E agora?&lt;/h2&gt;

&lt;p&gt;Dado que tenhamos encontrado os difeomorfimos o que podemos fazer com eles?&lt;/p&gt;

&lt;p&gt;Bom, uma aplicação bem famosa é forçar que os difeomorfismos tenham dimensões pequenas, como 2 ou 3, para que a variedade possa ser visualizada. Essa é uma aplicação de Redução de Dimensionalidade (&lt;a href=&quot;https://en.wikipedia.org/wiki/Dimensionality_reduction&quot;&gt;Dimensionality Reduction&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Podemos ainda aplicar uma espécie de filtro que permita observar pedaços da variedade no formato de grafos e como ali se comportam o valor das features. Isso faz parte de uma área de estudo conhecida como Análise Topológica de Dados (&lt;a href=&quot;https://en.wikipedia.org/wiki/Topological_data_analysis&quot;&gt;Topolical Data Analysis&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Na minha humilde opinião as aplicações mais interessantes se encontram em aprender a estrutura da variedade de modo que possamos fazer amostragem (sampling) de sua pontos. As técnicas usadas para fazer isso são aquelas que usam modelos geradores como &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_adversarial_network&quot;&gt;Generative Adversarial Networks &lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Autoencoder#Variational_autoencoder_(VAE)&quot;&gt;Variational Autoencoder&lt;/a&gt; e &lt;a href=&quot;http://akosiorek.github.io/ml/2018/04/03/norm_flows.html&quot;&gt;normalizing-flows&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Sobre as técnicas de manifold learning especificamente pretendo apresentar aqui futuramente.&lt;/p&gt;</content><author><name>Junior A. Koch</name></author><category term="information" /><category term="Kullback" /><category term="Leibler" /><category term="mutual information" /><category term="conditional information" /><summary type="html">Muito do trabalhado de um Cientista de Dados não está apenas em criar sistemas de classificação, regressão ou recomendação. Aliás, estes são normalmente o resultado do fim de uma grande jornada aplicando diversas técnicas em cima dos dados coletados. Uma importante parte dessa jornada está em descobrir maneiras de compreender e utilizar a estrutura dos dados.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/two_coordinate_charts_on_a_manifold.png" /></entry><entry><title type="html">Variational AutoEncoder</title><link href="http://localhost:4000/journal/variational-autoencoder.html" rel="alternate" type="text/html" title="Variational AutoEncoder" /><published>2020-05-10T00:00:00-03:00</published><updated>2020-05-10T00:00:00-03:00</updated><id>http://localhost:4000/journal/variational-autoencoder</id><content type="html" xml:base="http://localhost:4000/journal/variational-autoencoder.html">&lt;p&gt;Variational AutoEncoders or VAE are a class of generative models based on latent variables. Suppose we have our multidimensional data &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and we want to build a model from which we can sample data at least similar to &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;. We will make that with a multidimensional latent variable &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; to create a map &lt;script type=&quot;math/tex&quot;&gt;f:Z \to X&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;We need to model &lt;script type=&quot;math/tex&quot;&gt;p(X) = \int p(X \vert Z)dZ&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;p(X,Z) = p(X \vert Z)p(Z)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The ideia of VAE is to infer &lt;script type=&quot;math/tex&quot;&gt;p(Z)&lt;/script&gt;, but at first &lt;script type=&quot;math/tex&quot;&gt;p(Z \vert X)&lt;/script&gt; is unknown. To deal with that let us use a method called Variational Inference (VI). It is very popular together with Markov Chain Monte Carlo (MCMC) methods.&lt;/p&gt;

&lt;p&gt;We treat this as an optimization problem, we model &lt;script type=&quot;math/tex&quot;&gt;p(Z \vert X)&lt;/script&gt; using some distribution and minimize the Kullback-Liebler (KL) divergence between our chosen distribution, let us call it &lt;script type=&quot;math/tex&quot;&gt;q(Z \vert X)&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;p(Z \vert X)&lt;/script&gt;. We have&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;KL \left[q(Z \vert X)\vert \vert p(Z \vert X) \right] = \sum_{Z} q(Z \vert X) \log{\frac{q(Z \vert X)}{p(Z \vert X)}}  = -\sum_{Z} q(Z \vert X) \log{\frac{p(Z \vert X)}{q(Z \vert X)}}&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;KL \left[q(Z \vert X)\vert \vert p(Z \vert X) \right] = \mathbb{E}_{Z} \left[\log{\frac{p(Z \vert X)}{q(Z \vert X)}}\right] = \mathbb{E}_{Z} \left[\log{q(Z \vert X)} - \log{p(Z \vert X)}  \right]&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;and we use Bayes Theorem treating &lt;script type=&quot;math/tex&quot;&gt;p(Z \vert X)&lt;/script&gt; as a posterior: &lt;script type=&quot;math/tex&quot;&gt;p(Z \vert X) = \frac{p(X \vert Z)p(Z)}{p(X)}&lt;/script&gt; rewriting the KL divergence&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;KL[q(Z \vert X)\vert \vert p(Z \vert X)] = \mathbb{E}_{Z} \left[\log{q(Z \vert X)} - \log{p(X \vert Z)} -\log{p(Z)} + \log{p(X)}  \right]&lt;/script&gt;

&lt;p&gt;Since the expectation is over &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; the term &lt;script type=&quot;math/tex&quot;&gt;\log{p(X)}&lt;/script&gt; is constant and can be factored out. Looking closely we see another KL divergence inside the expectation&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathbb{E}_{Z} \left[\log{q(Z \vert X)} - \log{p(Z)} \right] = KL \left[q(Z \vert X)\vert \vert p(Z) \right]&lt;/script&gt;. Rearranging the equation we have the VAE objective function.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L} = \log{p(X)} - KL \left[q(Z \vert X)\vert \vert p(Z \vert X) \right] = \mathbb{E}_{Z} \left[\log{p(X \vert Z)} \right] - KL \left[q(Z \vert X)\vert \vert p(Z) \right]&lt;/script&gt;

&lt;p&gt;Let us see the meaning of all these distributions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;q(Z \vert X)&lt;/script&gt; is a function which projects &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; into latent space.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;p(X \vert Z)&lt;/script&gt; is a function which projects &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; into features space.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is common to say that &lt;script type=&quot;math/tex&quot;&gt;q(Z \vert X)&lt;/script&gt; encodes the information of &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;p(X \vert Z)&lt;/script&gt; do the opposite, decodes &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; back to &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;. In the ideal case we want the following diagram to commute&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/vae_identity_diagram.png&quot; style=&quot;float: left; margin-right: 10px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On summary, looking at &lt;script type=&quot;math/tex&quot;&gt;\mathcal{L} = \log{p(X)} - KL \left[q(Z \vert X)\vert \vert p(Z \vert X) \right]&lt;/script&gt; we want to model &lt;script type=&quot;math/tex&quot;&gt;\log{p(X)}&lt;/script&gt; setting an error &lt;script type=&quot;math/tex&quot;&gt;KL \left[q(Z \vert X)\vert \vert p(Z \vert X) \right]&lt;/script&gt;, i.e., the VAE tries to find a lower bound to &lt;script type=&quot;math/tex&quot;&gt;\log{p(X)}&lt;/script&gt;, which is intractable.&lt;/p&gt;

&lt;p&gt;The model can be found maximizing &lt;script type=&quot;math/tex&quot;&gt;\log{p(X \vert Z)}&lt;/script&gt; and minimizing the difference between &lt;script type=&quot;math/tex&quot;&gt;q(Z \vert X)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;p(Z)&lt;/script&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;MAX_{X,Z} VAE = \mathbb{E} \left[\log{p(X \vert Z)} \right] - KL \left[q(Z \vert X) \vert \vert p(Z) \right]&lt;/script&gt;

&lt;p&gt;Remember that maximizing &lt;script type=&quot;math/tex&quot;&gt;\mathbb{E} \left[\log{p(X \vert Z)} \right]&lt;/script&gt; is an estimation by maximum likelihood.&lt;/p&gt;

&lt;p&gt;There is only one question left. What kind of distribution should we use for &lt;script type=&quot;math/tex&quot;&gt;p(Z)&lt;/script&gt;? We can try a something simple like a normal distribution with zero mean and variance one, &lt;script type=&quot;math/tex&quot;&gt;\mathcal{N}(0,1)&lt;/script&gt;. Given &lt;script type=&quot;math/tex&quot;&gt;KL \left[q(Z \vert X) \vert \vert p(Z) \right]&lt;/script&gt;, we want &lt;script type=&quot;math/tex&quot;&gt;q(Z \vert X)&lt;/script&gt; to be as near as possible of &lt;script type=&quot;math/tex&quot;&gt;\mathcal{N}(0,1)&lt;/script&gt;. The good part of the choice is that we have a closed form for the KL divergence. Let us represent the mean by &lt;script type=&quot;math/tex&quot;&gt;\mu(X)&lt;/script&gt; and the variance by &lt;script type=&quot;math/tex&quot;&gt;\Sigma(X)&lt;/script&gt;. The KL divergence is (we computed this in the Information Theory post)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;KL \left[\mathcal{N}(\mu(X),\Sigma(X)) \vert \vert \mathcal{N}(0,1) \right] = \frac{1}{2} \sum_{k}\left(\Sigma(X)+ \mu^{2}(X) -1 -\log{\Sigma(X)} \right)&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; is the Gaussian dimension. You will see that in practice it is stable if we use &lt;script type=&quot;math/tex&quot;&gt;\Sigma(X)&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;\log{\Sigma(X)}&lt;/script&gt; and finally we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;KL \left[\mathcal{N}(\mu(X),\Sigma(X)) \vert \vert \mathcal{N}(0,1) \right] = \frac{1}{2} \sum_{k}\left(e^{\Sigma(X)}+ \mu^{2}(X) -1 -\Sigma(X) \right)&lt;/script&gt;</content><author><name>Junior A. Koch</name></author><category term="information" /><category term="Kullback" /><category term="Leibler" /><category term="mutual information" /><category term="conditional information" /><summary type="html">Variational AutoEncoders or VAE are a class of generative models based on latent variables. Suppose we have our multidimensional data and we want to build a model from which we can sample data at least similar to . We will make that with a multidimensional latent variable to create a map .</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/information_theory.jpg" /></entry><entry><title type="html">Information Theory</title><link href="http://localhost:4000/journal/information-theory.html" rel="alternate" type="text/html" title="Information Theory" /><published>2019-10-19T00:00:00-03:00</published><updated>2019-10-19T00:00:00-03:00</updated><id>http://localhost:4000/journal/information-theory</id><content type="html" xml:base="http://localhost:4000/journal/information-theory.html">&lt;p&gt;Suppose we have a random variable &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; and we are interested in how much information we get when &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is measured. If the event is highly improbable we have more information than if it is highly probable. Information is given by a quantity &lt;script type=&quot;math/tex&quot;&gt;h(x)&lt;/script&gt; that is a function of the probability distribution &lt;script type=&quot;math/tex&quot;&gt;p(x)&lt;/script&gt;. If two events are observed and unrelated the total information will be the sum of information in both events:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h(x,y) = h(x) + h(y)\ \iff \ p(x,y) = p(x)p(y)&lt;/script&gt;

&lt;p&gt;We see that information depends on the logarithm of the probability distribution. For a discrete &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h(x) = - log_{2}p(x).&lt;/script&gt;

&lt;p&gt;The expectation of this quantity is what we call &lt;code class=&quot;highlighter-rouge&quot;&gt;entropy&lt;/code&gt; of the variable &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H[x] = -  \sum_{x} p(x) log_{2}p(x)&lt;/script&gt;

&lt;p&gt;If there’s no chance o observing &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, i.e. &lt;script type=&quot;math/tex&quot;&gt;p(x)=0&lt;/script&gt;, then we get no information &lt;script type=&quot;math/tex&quot;&gt;H[x]=0&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;As an example, suppose we observe the throw of a fair 8-sided dice and we want to transmit the information about the expectation:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;3.0&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This is measured in &lt;code class=&quot;highlighter-rouge&quot;&gt;bits&lt;/code&gt;. In this case we need at least a 3 &lt;code class=&quot;highlighter-rouge&quot;&gt;bits&lt;/code&gt; number to transmit the information, no less than that. When &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is continuous it is common to use the natural logarithm and the unit becomes &lt;code class=&quot;highlighter-rouge&quot;&gt;nats&lt;/code&gt; instead of bits.&lt;/p&gt;

&lt;p&gt;Let us see an example of three distributions with the same mean but different standard deviation.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;p1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.005&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.99&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.005&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Define a function to compute entropy:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.6094&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.3762&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0629&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The narrower distribution has a smaller entropy. This is because the uncertainty about its expectation is smaller. If some &lt;script type=&quot;math/tex&quot;&gt;p(x_{i})=1&lt;/script&gt; all other &lt;script type=&quot;math/tex&quot;&gt;p(x_{j \ne i})=0&lt;/script&gt;, uncertainty is zero and we get no information &lt;script type=&quot;math/tex&quot;&gt;H(x_{i})=0&lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;differential-entropy&quot;&gt;Differential Entropy&lt;/h2&gt;

&lt;p&gt;We can use a continuous distribution, &lt;script type=&quot;math/tex&quot;&gt;p(x)&lt;/script&gt; and the discrete sum becomes an integral and the entropy is usually called &lt;code class=&quot;highlighter-rouge&quot;&gt;differential entropy&lt;/code&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H[x] = -\int p(x) \ln p(x) dx.&lt;/script&gt;

&lt;p&gt;We can now use &lt;code class=&quot;highlighter-rouge&quot;&gt;Lagrange multipliers&lt;/code&gt; to find a distribution which maximizes the differential entropy. To do that we need to constrain the first and second moments of &lt;script type=&quot;math/tex&quot;&gt;p(x)&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_{-\infty}^{+\infty}p(x)dx = 1,&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_{-\infty}^{+\infty} x p(x)dx = \mu,&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_{-\infty}^{+\infty} (x- \mu)^{2} p(x)dx = \sigma^{2}.&lt;/script&gt;

&lt;p&gt;Setting the &lt;code class=&quot;highlighter-rouge&quot;&gt;variational derivative&lt;/code&gt; to zero we get as solution &lt;script type=&quot;math/tex&quot;&gt;p(x) = \exp{(-1 + \lambda_{1} + \lambda_{2} x + \lambda_{3} (x - \mu)^{2})}&lt;/script&gt;. Using substitution we get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x) = \frac{1}{\sqrt{2 \pi \sigma^{2}}}\exp{\left(-\frac{(x - \mu)^{2}}{2 \sigma^{2}}\right)}&lt;/script&gt;

&lt;p&gt;The Gaussian distribution is the one that maximizes differential entropy which is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H[x] =  -\int_{-\infty}^{+\infty} \mathcal{N}(x\vert \mu, \sigma^{2}) \ln \mathcal{N}(x\vert \mu, \sigma^{2}) dx = \frac{1}{2}(1 + \ln{2 \pi \sigma^{2}})&lt;/script&gt;

&lt;p&gt;This result agrees with what we found earlier, the entropy increases as the distribution becomes broader, i.e., &lt;script type=&quot;math/tex&quot;&gt;\sigma^{2}&lt;/script&gt; increases.&lt;/p&gt;

&lt;h2 id=&quot;conditional-entropy&quot;&gt;Conditional Entropy&lt;/h2&gt;

&lt;p&gt;Suppose now we have a joint distribution &lt;script type=&quot;math/tex&quot;&gt;p(x,y)&lt;/script&gt; and have an observation of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;. It is possible to compute the additional information needed to specify the corresponding observation of &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;- \ln p(y\vert x).&lt;/script&gt; Thus the average additional information to specify &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;, called &lt;code class=&quot;highlighter-rouge&quot;&gt;conditional entropy&lt;/code&gt; of &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; given &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H[y\vert x] = -\int \int p(y,x) \ln{ p(y\vert x)}dydx&lt;/script&gt;

&lt;p&gt;Using &lt;script type=&quot;math/tex&quot;&gt;p(y\vert x) = \frac{p(y,x)}{p(x)}&lt;/script&gt; we get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H[y\vert x] = H[y,x] - H[x].&lt;/script&gt;

&lt;h2 id=&quot;kullback-leibler-divergence&quot;&gt;Kullback-Leibler Divergence&lt;/h2&gt;

&lt;p&gt;Suppose we have an unknown distribution &lt;script type=&quot;math/tex&quot;&gt;p(x)&lt;/script&gt; and we are using another distribution &lt;script type=&quot;math/tex&quot;&gt;q(x)&lt;/script&gt; to model it. We can compute the additional amount of information needed to specify &lt;script type=&quot;math/tex&quot;&gt;x \sim p(x)&lt;/script&gt; when we observe &lt;script type=&quot;math/tex&quot;&gt;x \sim q(x)&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;KL(p\vert \vert q) = -\int p(x) \ln q(x) dx + \int p(x) \ln p(x) dx = -\int p(x) \ln{\left(\frac{q(x)}{p(x)} \right)}dx.&lt;/script&gt;

&lt;p&gt;This &lt;code class=&quot;highlighter-rouge&quot;&gt;relative entropy&lt;/code&gt; is called &lt;code class=&quot;highlighter-rouge&quot;&gt;Kullback-Leibler Divergence&lt;/code&gt; or simply &lt;code class=&quot;highlighter-rouge&quot;&gt;KL Divergence&lt;/code&gt; and is a measure of dissimilarity between two distributions. Note that it is anti-symmetric,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;KL(p\vert \vert q) \ne KL(q\vert \vert p).&lt;/script&gt;

&lt;p&gt;To accomplish the task of approximating &lt;script type=&quot;math/tex&quot;&gt;q(x)&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;p(x)&lt;/script&gt; we may observe &lt;script type=&quot;math/tex&quot;&gt;x \sim p(x)&lt;/script&gt; a finite number of times, &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt;, use &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; as a parametric function, &lt;script type=&quot;math/tex&quot;&gt;q(x \vert \theta)&lt;/script&gt; and use the expectation of &lt;script type=&quot;math/tex&quot;&gt;KL(p \vert \vert q).&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;For a function &lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt; its expectation is &lt;script type=&quot;math/tex&quot;&gt;\mathbb{E}[f] = \int p(x) f(x)dx&lt;/script&gt; and for a &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; number of observations it becomes a finite sum:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}[f] \approx \frac{1}{N} \sum_{n=1}^{N} f(x_{n}).&lt;/script&gt;

&lt;p&gt;For the KL divergence we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;KL(p \vert \vert q) \approx \frac{1}{N} \sum_{n=1}^{N}(-\ln q(x_{n} \vert \theta) + \ln p(x_{n})).&lt;/script&gt;

&lt;p&gt;The first therm is the &lt;code class=&quot;highlighter-rouge&quot;&gt;negative log likelihood&lt;/code&gt; for &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; under distribution &lt;script type=&quot;math/tex&quot;&gt;q(x\vert \theta)&lt;/script&gt;. Because of that people usually say that &lt;code class=&quot;highlighter-rouge&quot;&gt;minimizing the KL divergence is equivalent to maximizing the likelihood function&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;As an exercise we can find &lt;script type=&quot;math/tex&quot;&gt;KL(p\vert \vert q)&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;p(x)=\mathcal{N}(x\vert \mu, \sigma^{2})&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;q(x)=\mathcal{N}(x\vert m, s^{2})&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;KL(p\vert \vert q) = -\int p(x) \ln{\frac{q(x)}{p(x)}}dx&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;KL(p\vert \vert q) = -\int p(x) \ln{q(x)}dx + \int p(x) \ln{p(x)}dx&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;KL(p\vert \vert q) = \frac{1}{2}\int p(x) \ln{2 \pi s^{2}}dx + \frac{1}{2s^{2}}\int p(x)(x-m)^{2}dx - \frac{1}{2}(1+\ln{2 \pi \sigma^{2}})&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;KL(p\vert \vert q) = \frac{1}{2}\ln{2 \pi s^{2}} - \frac{1}{2}(1+\ln{2 \pi \sigma^{2}})+ \frac{1}{2s^{2}}(\langle x \rangle^{2} -2m \langle x \rangle + m^{2})&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;KL(p\vert \vert q) = \frac{1}{2} \ln{\frac{s^{2}}{\sigma^{2}}} - \frac{1}{2} + \frac{\sigma^{2}+(\mu - m)^{2}}{2s^{2}}&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;KL(p\vert \vert q) = \ln{\frac{s}{\sigma}} - \frac{1}{2} + \frac{\sigma^{2}+(\mu - m)^{2}}{2s^{2}},&lt;/script&gt;&lt;br /&gt;
where &lt;script type=&quot;math/tex&quot;&gt;(\langle x \rangle -m)^{2} -\langle x \rangle^{2} = -2 \langle x \rangle m + m^{2}&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;f-divergence&quot;&gt;f-Divergence&lt;/h3&gt;

&lt;p&gt;The KL divergence is the most famous function of a broader family called &lt;code class=&quot;highlighter-rouge&quot;&gt;f-Divergences&lt;/code&gt;, more generally defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D_{f}(p \vert \vert q) \equiv \frac{4}{1 - f^{2}} \left( 1 - \int p(x)^{\left(\frac{1 + f}{2} \right)} q(x)^{\left(\frac{1 - f}{2} \right)}dx \right)&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is a continuous real parameter, &lt;script type=&quot;math/tex&quot;&gt;-\infty \le f \le + \infty&lt;/script&gt;. Some special cases are:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;KL(p\vert \vert q) = \lim_{f \rightarrow 1} D_{f}(p \vert \vert q),&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;KL(q\vert \vert p) = \lim_{f \rightarrow -1} D_{f}(p \vert \vert q)&lt;/script&gt;

&lt;p&gt;and the &lt;code class=&quot;highlighter-rouge&quot;&gt;Hellinger distance&lt;/code&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D_{H}(p \vert \vert q) = \lim_{f \rightarrow 0}  D_{f}(p \vert \vert q) = \int \left( p(x)^{2} - q(x)^{2} \right)^{2}dx.&lt;/script&gt;

&lt;p&gt;Since we only work with distributions with compact support &lt;script type=&quot;math/tex&quot;&gt;D_{f}(p \vert \vert q) \ge 0&lt;/script&gt;, again it is zero if and only if &lt;script type=&quot;math/tex&quot;&gt;p(x) = q(x)&lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;mutual-information&quot;&gt;Mutual Information&lt;/h2&gt;

&lt;p&gt;For a joint distribution &lt;script type=&quot;math/tex&quot;&gt;p(y,x)&lt;/script&gt; the KL divergence can be used to quantify how close the two variables are to be independent, i.e., &lt;script type=&quot;math/tex&quot;&gt;p(y,x) = p(y)p(x)&lt;/script&gt;, this is called &lt;code class=&quot;highlighter-rouge&quot;&gt;mutual information&lt;/code&gt; between &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;I[y,x] = KL(p(y,x)\vert \vert p(y)p(x)) = -\int \int p(y,x) \ln \frac{p(y)p(x)}{p(y,x)}dydx&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;I[y,x] \ge 0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;I[y,x] = 0 \iff&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; are independent.&lt;/p&gt;

&lt;p&gt;Mutual information can be written with conditional entropy:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;I[y,x] = H[y] - H[y\vert x] = H[x] - H[x\vert y]&lt;/script&gt;.&lt;/p&gt;</content><author><name>Junior A. Koch</name></author><category term="information" /><category term="Kullback" /><category term="Leibler" /><category term="mutual information" /><category term="conditional information" /><summary type="html">Suppose we have a random variable and we are interested in how much information we get when is measured. If the event is highly improbable we have more information than if it is highly probable. Information is given by a quantity that is a function of the probability distribution . If two events are observed and unrelated the total information will be the sum of information in both events:</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/information_theory.jpg" /></entry><entry><title type="html">Counter Factual Model</title><link href="http://localhost:4000/journal/counter-factual_model.html" rel="alternate" type="text/html" title="Counter Factual Model" /><published>2019-09-01T00:00:00-03:00</published><updated>2019-09-01T00:00:00-03:00</updated><id>http://localhost:4000/journal/counter-factual_model</id><content type="html" xml:base="http://localhost:4000/journal/counter-factual_model.html">&lt;h1 id=&quot;inferência-causal&quot;&gt;Inferência Causal&lt;/h1&gt;

&lt;p&gt;É comum a confusão entre causa e associação e isso pode levar a muitas interpretações errôneas. Supomos que &lt;em&gt;X&lt;/em&gt; e &lt;em&gt;Y&lt;/em&gt; representam a distribuição de duas variáveis aleatórias, &lt;em&gt;x&lt;/em&gt; e &lt;em&gt;y&lt;/em&gt;. Ao afirmar “&lt;em&gt;X causa Y&lt;/em&gt;” estamos dizendo que mudar o valor de &lt;em&gt;X&lt;/em&gt; irá mudar o valor da distribuição &lt;em&gt;Y&lt;/em&gt;. Se “&lt;em&gt;X causa Y&lt;/em&gt;”, &lt;em&gt;X&lt;/em&gt; e &lt;em&gt;Y&lt;/em&gt; estão associados, mas o contrário não é necessariamente verdade. Vamos analisar sobre maneiras de abordar causalidade: modelos contrafactuais e modelos gráficos.&lt;/p&gt;

&lt;h2 id=&quot;modelo-contrafactual&quot;&gt;Modelo Contrafactual&lt;/h2&gt;

&lt;p&gt;Usaremos o dataset abaixo no qual temos algumas características de produtos de um e-Commerce juntamente com sua probabilidade de venda.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'data/counter_factual_data_example.csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Sold'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Selling_probability'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Neste dataset alguns produtos foram vendidos com frete grátis, &lt;code class=&quot;highlighter-rouge&quot;&gt;Free_Shipment = 1&lt;/code&gt; e outros não &lt;code class=&quot;highlighter-rouge&quot;&gt;Free_Shipment = 0&lt;/code&gt;. Estamos interessados em saber se oferecer frete grátis aumenta a probabilidade de venda de um produto, ou seja, &lt;code class=&quot;highlighter-rouge&quot;&gt;Free_Shipment&lt;/code&gt; causa &lt;code class=&quot;highlighter-rouge&quot;&gt;Sold&lt;/code&gt;. Os dados que possímos são dados observados e portanto não podemos voltar no tempo e oferecer frete grátis e observar novamente.&lt;/p&gt;

&lt;p&gt;Usaremos &lt;code class=&quot;highlighter-rouge&quot;&gt;Free_Shipment&lt;/code&gt; como a variável binária &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; e &lt;code class=&quot;highlighter-rouge&quot;&gt;Sold&lt;/code&gt; como a variável de saída &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;. É comum chamar &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; de variável de tratamento e essa abordagem de &lt;em&gt;efeito pós tratamento&lt;/em&gt;. Então precisamos diferenciar as afirmações:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;“&lt;em&gt;X&lt;/em&gt; causa &lt;em&gt;Y&lt;/em&gt;”;&lt;/li&gt;
  &lt;li&gt;“&lt;em&gt;X&lt;/em&gt; está associado a &lt;em&gt;Y&lt;/em&gt;”.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Para isso vamos utilizar duas variáveis aleatórias chamadas de &lt;strong&gt;potenciais resultados&lt;/strong&gt;. São elas:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;C_{0}&lt;/script&gt; - o resultado se &lt;script type=&quot;math/tex&quot;&gt;X=0&lt;/script&gt;;&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;C_{1}&lt;/script&gt; - o resultado se &lt;script type=&quot;math/tex&quot;&gt;X=1&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Assim, &lt;script type=&quot;math/tex&quot;&gt;Y = C_{0}&lt;/script&gt; se &lt;script type=&quot;math/tex&quot;&gt;X = 0&lt;/script&gt; e &lt;script type=&quot;math/tex&quot;&gt;Y = C_{1}&lt;/script&gt; se &lt;script type=&quot;math/tex&quot;&gt;X = 1&lt;/script&gt;, ou ainda a chamada &lt;strong&gt;relação de consistência&lt;/strong&gt;,&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Y = C_{X}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Vamos fazer isso no nosso dataset.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'C0'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Free_Shipment'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Sold'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'C1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Free_Shipment'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Sold'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Free_Shipment'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Sold'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'C0'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'C1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Os &lt;code class=&quot;highlighter-rouge&quot;&gt;NaN&lt;/code&gt; indicam valores não observados. Se &lt;script type=&quot;math/tex&quot;&gt;X=0&lt;/script&gt; não temos como observar &lt;script type=&quot;math/tex&quot;&gt;C_{1}&lt;/script&gt;, por isso diz-se que &lt;script type=&quot;math/tex&quot;&gt;C_{1}&lt;/script&gt; é &lt;strong&gt;contrafactual&lt;/strong&gt; já que ele representa qual seria o resultado, contra os fatos, se &lt;script type=&quot;math/tex&quot;&gt;X=1&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Estas novas variáveis podem ser vistas como variáveis ocultas encontradas em outros modelos.&lt;/p&gt;

&lt;p&gt;Agora podemos definir o &lt;strong&gt;efeito causal médio&lt;/strong&gt; ou &lt;strong&gt;efeito de tratamento médio&lt;/strong&gt;. Este é dado por&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta  = \mathbb{E}(C_{1})-\mathbb{E}(C_{0})&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Podemos interpretar &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; como o valor esperado dos resultados caso todos os produtos tivessem &lt;script type=&quot;math/tex&quot;&gt;X=1&lt;/script&gt; menos o valor esperado dos resultados caso todos os produtos tivessem &lt;script type=&quot;math/tex&quot;&gt;X=0&lt;/script&gt;. Há algumas maneiras de medir o efeito causal, por exemplo, se &lt;script type=&quot;math/tex&quot;&gt;C_{1}&lt;/script&gt; e &lt;script type=&quot;math/tex&quot;&gt;C_{0}&lt;/script&gt; são binários usualmente define-se a razão causal provável&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\mathbb{P}(C_{1}=1)}{\mathbb{P}(C_{1}=0)} \div \frac{\mathbb{P}(C_{0}=1)}{\mathbb{P}(C_{0}=0)}&lt;/script&gt;

&lt;p&gt;e o risco causal relativo&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\mathbb{P}(C_{1}=1)}{\mathbb{P}(C_{0}=1)}&lt;/script&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# efeito causal médio
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ecm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'C1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'C0'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# razao causal provável
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_c1_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'C1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p_c1_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_c1_1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p_c0_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'C0'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p_c0_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_c0_1&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;rcp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_c1_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_c1_0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_c0_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_c0_0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# risco causal relativo
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_c1_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_c0_1&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ECM: {:.3f}| RCP: {:.3f}| RCR: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ecm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Já a associação é definida como&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha = \mathbb{E}(Y|X=1) - \mathbb{E}(Y|X=0),&lt;/script&gt;

&lt;p&gt;ou seja, o valor esperado de &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; dado que &lt;script type=&quot;math/tex&quot;&gt;X=1&lt;/script&gt; menos o valor esperado de &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; dado que &lt;script type=&quot;math/tex&quot;&gt;X=0&lt;/script&gt;. Vejamos a associação para nossos dados:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;association&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Free_Shipment'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Sold'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Free_Shipment'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Sold'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Association: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;association&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Verificamos uma associação positiva, ou seja, oferecer frete &lt;script type=&quot;math/tex&quot;&gt;X=1&lt;/script&gt; grátis aumenta as chances de venda, mas a associação não é igual ao efeito causal médio, como geralmente é o caso.&lt;/p&gt;

&lt;p&gt;O que fazemos em machine learning usando aprendizado supervisionado está voltado à associação. Coletamos observações independentes umas das outras e encontramos uma função que generaliza as distribuições das features para encontrar &lt;script type=&quot;math/tex&quot;&gt;p(Y\|X_{1},...,X_{n})&lt;/script&gt;. Para entender a causalidade precisamos encontrar algo diferente, &lt;script type=&quot;math/tex&quot;&gt;p(Y\|X_{1}=x,...,X_{n})&lt;/script&gt;, a distribuição de &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; dado que &lt;script type=&quot;math/tex&quot;&gt;X_{1}&lt;/script&gt; tem valor igual a &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; para todas as observações.&lt;/p&gt;</content><author><name>Junior A. Koch</name></author><category term="causal" /><category term="inference" /><category term="counter" /><category term="factual" /><summary type="html">Inferência Causal</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/whatif.jpg" /></entry></feed>