---
layout: post
title: "Directed Graphs and Conditional Independence"
author: "Junior A. Koch"
categories: journal
tags: [information, Kullback, conditional information]
image: network.jpg
---

## The Bias-Variance Tradeoff

Let $g$ denote a function and $\hat{g}_{n}$ denote an estimator of $g$. $\hat{g}_{n}(x)$ is a random function evaluated at $x$. The loss is the integrated squared error (ISE):

$L(g,\hat{g}_{n}) = \int \left( g(u) - \hat{g}_{n}(u) \right)^{2}du$

The risk or mean integrated squared error (MISE) with respect to squared error loss is

$R(f,\hat{f} = \mathbb{E}\left[ L(g,\hat{g}_{n}) \right]$

In summary: $RISK = BIAS^{2} + VARIANCE$

## Histograms

Let $X_{1}, ..., X_{n}$ be iid on $[0, 1]$ with density $f$. Let $m$ be an integer and define bins:

$B_{1} = \left[ 0, \frac{1}{m} \right),\ B_{2} = \left[ \frac{1}{m}, \frac{2}{m} \right),\ ...,\ B_{m} = \left[ \frac{m-1}{m}, 1 \right)$

Define the binwidth $h = 1\m$, let $\nu_{j}$ be the number of observations in $B_{j}$, let $\hat{p}_{j} = \nu_{j}/n$ and let $p_{j} = \int_{B_{j}}f(u)du.

